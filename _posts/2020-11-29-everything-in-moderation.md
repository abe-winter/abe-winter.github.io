---
layout: post
title: Only distributed fact-checking can keep up with democratized distribution 
author: Abe Winter
description: Tragedy of the comments
categories: only fans
new: true
---

![tweet: the problem with being faster than light is that you always live in darkness]({{ "/assets/swift-ftl.png" | absolute_url }})

Has social media democratized reporting as promised?
We've democratized *writing and publishing* but this year has made the point that there's more to journalism than hitting send.
The rest of the stuff that happens in the newsroom has stayed in the newsroom:
cultivating sources, editing for quality, and most importantly, fact-checking.

Lacking checks, social media is amplifying or at least fertilizing false beliefs,
and then people cross state lines to shoot up pizza joints.
If the newsroom's adversarial functions can't make the jump to social media,
life will continue to be this weird.

We're starting to address fact-checking, and will do more.
[Failing to curb antisocial behavior kills companies](https://arstechnica.com/tech-policy/2019/11/why-cant-internet-companies-stop-awful-content/), as well as communities.

Facebook has the technology to filter out the worst-quality content that their users think is 'P(bad for the world)' -- [but only at the cost of lower engagement](https://twitter.com/kevinroose/status/1331257332538359810).
Given these incentives, nobody benefits if we leave this to platforms to solve.
Users should bring their own moderation to the web.

* toc
{:toc}

## Private groups will decamp if they don't like the fact bubbles

Filter bubbles don't have to put up with fact bubbles.
By filter bubbles, I mean clique groups that see mostly content that supports their priors.
By fact bubbles, I mean those blue exclamation marks that say 'official sources have called the election differently'.

Private groups have the power to resist centralized moderation if they don't like it.
Private groups are sticky which is why facebook [bet their future on them](https://www.theverge.com/2019/4/30/18524188/facebook-f8-keynote-mark-zuckerberg-privacy-future-2019),
but this stickiness makes them hard to control.
Yes, their internal cohesion leads to engagement on FB, but it also allows them to move off the platform if they're unhappy.
They own their slice of the network; their moat has a drawbridge.

A heavy hand will drive private groups to dark corners of the web --
not *un*-moderated exactly, because trust me, nobody wants that.
But 'differently moderated'.
Reality is a participation sport and groups which don't want to participate, won't.

That said, deplatforming isn't zero-cost for groups:
they recruit in public spaces, and are helped by 'suggested groups' features (which FB [partially disabled](https://www.buzzfeednews.com/article/ryanmac/facebook-suspended-group-recommendations-election) for the election).
Standalone forums can't grow like that.
The fickle graces of [hosts](https://www.theverge.com/2019/8/5/20754943/8chan-epik-offline-voxility-service-cutoff-hate-speech-ban) and [DDOS-protection companies](https://www.theverge.com/2019/8/4/20754310/cloudflare-8chan-fredrick-brennan-ddos-attack) also make it hard to go solo.

But it's not like IRL networks will stop feeding online groups.
The counterargument to 'social media gives bad politicians a voice' is that once someone has been elected, they'll probably [have a voice anyway](https://www.youtube.com/watch?v=f7131IkiSCg).
FB's design gives eyeballs to demagogues but [so did Leni Riefenstahl](https://en.wikipedia.org/wiki/Triumph_des_Willens).

As I drafted this in early november, twitter was fact-bubbling multiple tweets in a row from the president,
the 'uncensored' social media app parler [trended to #1](https://twitter.com/yashar/status/1325229370126991363) in apple's store,
and local hero [Bill Shatner took up the cause of moderation](https://twitter.com/WilliamShatner/status/1326223481646665728) to oppose them.
The 'moderation drives exodus' hypothesis is plausible.

But parler can't really want to be unmoderated.
Their debut came with a quick correction that despite the sales pitch, [your username can't be cumdumpster](https://twitter.com/viaCristiano/status/1277941967402553345).
([Twitter has allowed this](https://twitter.com/cumdumpster) since at least 2013).

My point: nobody wants to live in a trash can.
Every community has some kind of semi-official management.
I think even 8kun revoked someone's access to the Q account after they had a falling out (admittedly, not over content I think, and also I don't really understand this world).
But the *forms* of moderation we invent have to be trusted or at least tolerated, or people will ditch.

Fact-checking, if it's trustworthy and useful, will be embraced IMO. But it can't be imposed.

## Conspiracies are topics too

A swedish friend, trying to explain why their [far-right party](https://en.wikipedia.org/wiki/Sweden_Democrats) gained power in the Riksdag,
said something like 'It's not that we're all nazis, but nobody else would even talk about immigration, and it's a real problem'.

I like the 'marketplace of ideas' as a concept.[^marketplace]
Markets are an interesting model for public discourse.
Like goods on markets, there are a limited number of explanations or ideologies available (assuming most people can't coin our own -- for various reasons, I believe we can't).
Also, like a real market, it's two-sided, in that in addition to the ideas on offer, there are buyers:
people looking for an action, belief, or political party that suits them.

[^marketplace]: I recently found out this metaphor, if not the exact phrase, probably traces back to SCOTUS judge Oliver Wendell Holmes. He used it in his famous dissent in Abrams. The other justices *went to his house* before he published it and [tried to change his mind](https://www.theatlantic.com/national/archive/2013/08/the-most-powerful-dissent-in-american-history/278503/) on the grounds that unchecked speech would poison the nation. He also had an important 'but' in there: "we should be eternally vigilant against attempts to check the expression of opinions that we loathe and believe to be fraught with death, unless they so imminently threaten immediate interference with the lawful and pressing purposes of the law that an immediate check is required to save the country."

The marketplace model explains why political parties get to lie out loud, be revealed for hypocrisy, and admit their strategy is disingenuous.
They're *still* the only rallying point for 'critical idea X' that their buyers need.
It may not matter to the fighting faithful if the center is hollow.
They're willing to ignore the cognitive dissonance.

I wonder if there's a similar effect in the antivax movement --
autism seems to really be on the rise (0.7 to 1.5% in this century),
and if your family is affected by this, and there's no other explanation, you 'buy' the idea that comes with a clear cause and a solution.

## Cranky boomers R us

People are seeking answers that are plausible to them,
but may not be able or willing to dig deeply, and may not be ready to hear the truth if it means abandoning their hope, identity, or [existing opinions](https://phys.org/news/2020-11-beliefs-world-filter.html).
I suspect if someone is at the bottom of a rabbit hole and sees a fact bubble saying 'this is stupid', that's the same as saying 'you're stupid', and they won't listen.
The challenge here is matching them with a well-meaning authority whom they'll trust and find useful.

There was a measles outbreak in Minnesota enabled by high vaccine refusal rate in a single immigrant community.
Parents noticed a lot of their kids in special autism education[^stat-antivax], then fell prey to antivax lecturers or something.
But the parents had access to doctors.
And presumably they had access to other experts at the autism schools.
They ignored the fact bubble.
They chose who to trust.

[^stat-antivax]: [Measles sweeps an immigrant community targeted by anti-vaccine activists](https://www.statnews.com/2017/05/08/measles-vaccines-somali/), helen branswell, statnews

On the flip side, community-owned moderation sometimes works rather well.
I'm thinking about subreddits, who tend to have very specific rules about what to post, and their users accept that these communities are actively moderated for the benefit of the community.
(Given that the community is a result of moderation, this is a little circular and feedback loop-y, but that may not be a bad thing).

I asked a friend who mods a small subbredit what it's like:
"In six years, no one has ever said anything mean on my subreddit. I've never had to ban anyone and submitters and subscribers are respectful."

Cranky boomers got us into this mess by believing everything they read on facebook.
Maybe Cranky-Boomers-R-Us™️ is the right branding for a community fact check to filter out some fast-spreading egregious claims.

We train people to consider the source, but mainstream mastheads like NYT feel ideologically hostile to readers who aren't centrists over 55.
Their news coverage can be accurate without being neutral -- factual claims in articles are fact-checked, but that's mixed with interpretive conclusions.
The op-eds are [far stranger](https://www.cjr.org/the_media_today/james_bennet_tom_cotton_objectivity.php).

I don't blame people for switching to ideologically comfortable news.
I just wish they had a fact-check that was ideologically comfortable, but also competent and reputation-bound, to go along with it.

Repentant neocon Francis Fukuyama writes about social trust now and is [worried about post-fact world](https://www.project-syndicate.org/onpoint/the-emergence-of-a-post-fact-world-by-francis-fukuyama-2017-01) where
"all authoritative information sources are challenged by contrary facts of dubious quality and provenance".
Readers have reasons for straying from reality.
Let's give people an ideologically comfortable way to come back.

[Gentler public health messaging](https://en.wikipedia.org/wiki/Harm_reduction) that accepts people's lifestyles can have higher efficacy;
I suspect the same is true of fact-checks.
Our goal isn't to change minds; it's to improve outcomes.

## BYO vs centralized moderation

We need a market ecosystem of moderation vendors who provide different kinds of value to different kinds of users.
A moderation marketplace is good for platforms because they don't want to be the arbiter of truth, they just want users to have a good experience.
(Platforms will still have some policing to do on their own).

It's good for users because they're not forced into anything; they can pick which web they want to dwell on.
This may sound like a filter bubble, where 'everyone has their own truth',
but moderation vendors will still need to maintain their reputation for being right --
whatever their ideological alignment.
Moderation vendors, unlike cable news channels, don't produce content;
they just react to it and grade it.
BYO moderation isn't BYO newspaper, it's BYO snopes / politifact.

Centralized moderation has problems:

* The costs are asymmetric, meaning it's too expensive to do widely, well, and fast (see lego point below). Jimmy Wales of wikipedia describes the economics here leading to a [sweatshop model](https://medium.com/conversations-with-tyler/jimmy-wales-tyler-cowen-wikipedia-610b6e931d20).
* It's susceptible to abuse by platforms and institutions and therefore will be less trusted by users, especially the more conspiracy-minded ones whom we want to deradicalize. Note I'm not saying 'prone to abuse' or 'untrustworthy'. Just less trust-*ed*, and less able to balance minority viewpoints in a way that feels authentic to minorities.
* Private groups will turn off mod systems if not effective / ergonomic / trustable, or else deplatform.
* For messy issues, cookie-cutter moderation can [support violence by hiding its proof](https://www.eff.org/wp/caught-net-impact-extremist-speech-regulations-human-rights-content). Or like [where's the parler for breastfeeding](https://twitter.com/KatMurti/status/1276183164109697028)?

Every community needs moderation.
Some get away without it because they moderate membership rather than content --
small workplaces, for example, have relatively informal content moderation processes, but can fire you.

This isn't just a facebook / twitter issue anymore -- it's a problem for anyone with a supply of eyeballs and UGC or DM features, like [pinterest and linkedin](https://www.washingtonpost.com/technology/2020/11/07/pinterest-linkedin-election-disinfo/).
The older and funnier version of this is the [lego MMO's penis problem](https://www.managingcommunities.com/2015/06/04/lego-universes-penis-problem-and-why-moderation-efforts-arent-hopeless/) --
they scrapped their MMO *specifically and entirely* because of the cost of moderation.
A section 230 expert, talking about what happens in case of a repeal, observes that [sites which don't do *any* moderation in 2020 are unusable](https://twitter.com/jkosseff/status/1332655935114895360).

The need to moderate isn't a new problem.
Philosopher of science Karl Popper coined [Popper's paradox](http://www.openculture.com/2019/03/does-democracy-demand-the-tolerance-of-the-intolerant-karl-poppers-paradox.html) while thinking about the risks of tolerating intolerant views, and the parsing of arguments that happens when bad people ask for equal time.
Our problems wouldn't surprise him.

BYO is better because:

* The costs are borne by users. The model can be crowd-funded research -- for example, 10 people have offered $5 each to check a claim, that makes it worthwhile for a researcher to take on.
* The dispute & appeal processes can be more transparent and community-owned. I know people working on safety systems for dating apps, and the user feedback driving this is that the report process is opaque, dissatisfying, and ineffective. It's the same on social.
* Your ability to shop for a vendor leads to a customer service mentality by vendors. It also makes users view fact-checking as a utility, i.e. something useful that they call on to resolve doubt.
* Ideological alignment and factual reputation will become separate axes. Even if I'm only looking for party-approved fact-checkers, I can still pick the best one.
* The plugin architecture can support moderation workflows that don't exist today -- for example, [voluntary and amplified retraction](https://nickpunt.com/blog/deescalating-social-media/).

Decentralized moderation is starting to exist in decentralized platforms, though it's not exactly BYO.
This [knight foundation article on mastodon](https://knightcolumbia.org/content/what-if-social-media-worked-more-like-email) talks about self-hosted communities experimenting with alternative moderation rules.
(The examples, gab + lolicon, aren't inspiring).
A proposed [distributed reputation system for matrix](https://www.matrix.org/blog/2020/10/19/combating-abuse-in-matrix-without-backdoors/) uses external reputation feeds that you can subscribe to.
Whatsapp is [testing out meedan's 'check' tool](https://www.poynter.org/reporting-editing/2019/here-comes-a-tool-approved-by-whatsapp-to-automate-the-distribution-of-fact-checks/) which has some hybrid human / computer stuff but seems to not be community-driven.

## Information also has professional customers

> Complex and open-minded thought is most likely to be activated when decision makers learn prior to forming any opinions that they will be accountable to an audience (a) whose views are unknown, (b) who is interested in accuracy, (c) who is reasonably well-informed, and (d) who has a legitimate reason for inquiring into the reasons behind participants’ judgments/choices.

-- Philip Tetlock summarized in Annie Dukes' Thinking in Bets

Misinformation online is often presented as a problem of preventing bad decisions based on bad information --
don't cross state lines to shoot up a pizza joint, don't skip your vaccine.
But there are people who are trying to make good decisions who need good information.

There are topics like flat earth theory, where there's no real controversy over what's at issue.
But there's also dark matter, where observational evidence is hard to get and nobody is quite sure why galaxies rotate the way they do.
As a non-astronomer, a better-moderated internet would help me to stay up to date on, say, MOND, an alternate explanation of galaxy-scale physics.

I read about the 1920s [Shapley-Curtis Debate](https://en.wikipedia.org/wiki/Great_Debate_(astronomy)) over 'spiral nebulae':
were they tiny suburbs of the milky way, or 'island universes' (a term coined by Kant), equal in scale and very far away.
Shapley came armed with observations, later proven false, that the messier object M101 was rotating at a speed that would violate the speed of light were it very far.
This wasn't settled until Hubble studied cepheid variables a few years later.

Flat earthers have experiments on their website you can try yourself.
I heard an interview with someone from their group where he said "the flat earth society places an emphasis on personally experienced evidence".
As if [some polymath](https://en.wikipedia.org/wiki/Eratosthenes#Measurement_of_the_Earth's_circumference) hadn't proven this to our satisfaction millions of years ago.

Your time and expertise are limited.
You [can’t get a master’s degree in everything](https://acesounderglass.com/2019/10/17/what-comes-after-epistemic-spot-checks/) interesting or relevant to your life.
Nobody has time to seek independent truth; even [experts in ancient democracy](https://twitter.com/JoshData/status/1328396037267083266) are trusting sources.

I really believe everyone has some interest in and value for correct information.
But organizations that make large bets elevate truth-seeking as a core function.
(For example, and not to elevate ray dalio who is doing a book tour and seems like a windbag, but bridgewater's culture of radical transparency).
Orgs that suppress internal truth-finding get turned into salad by the competition.
Political scientist Caitlin Talmadge writes about how [this phenomenon can hobble armies]({% post_url 2020-06-04-dictators-army %}).

If we build tools to help filthy causals like me have useful + realistic beliefs about the world,
the same tools will help information professionals have *up-to-date* beliefs.
I have a book waiting on my kindle about the half life of facts which claims that compared to 40 years ago, valid beliefs become invalid more quickly.
Tooling to help us realign when that happens would make us stronger decision-makers, and may be similar in broad outline to BYO moderation tools.

In the same way that I have a 'like' button, I might like a 'believe' button that lets me mark a stat or claim I'm taking to heart,
not publicly,
but so that I can subscribe to retractions, updates, explanations.

To forward plan you have to have beliefs.
It's better if those beliefs are correct.
'A thing you believed is now false' should be the highest-ranked update on my timeline.

## Conclusions

> Every year, if not every day, we have to wager our salvation upon some prophecy based upon imperfect knowledge

-- Oliver Wendell Holmes

> But this is not all which distinguishes doubt from belief. There is a practical difference. Our beliefs guide our desires and shape our actions. The Assassins, or followers of the Old Man of the Mountain, used to rush into death at his least command, because they believed that obedience to him would insure everlasting felicity. Had they doubted this, they would not have acted as they did. So it is with every belief, according to its degree. The feeling of believing is a more or less sure indication of there being established in our nature some habit which will determine our actions. Doubt never has such an effect.

-- Peirce, [The Fixation of Belief](http://www.peirce.org/writings/p107.html)

Information quality is absolutely a life or death issue:
in questions of public health, in questions of war, in motivating lynch mobs.

I saw a [tweet from a favorite blogger](https://twitter.com/danluu/status/1271950429870387200) that, for me at least, summarized a feeling I had about public information in 2020:

> the info most people have seems to be 4-N months behind what laypeople with 3 hours+pubmed knew in January.

> The thing i wouldn't have guessed in 2003 is that informative indie bloggers would be rendered irrelevant by "social" algorithms that funnel people to clickbait.

When we say information is 'viral' online we're talking about the pace and pattern of its spread,
but I wonder if there's another way that information is viral on social media:
when it's packed into a small capsule, and doesn't contain its full argument, it can only reproduce by destroying its host.

Thinking about just the user-facing features of social media, it's 99% spread + engagement features now, with a few moderation and correctness features in the form of 'report' buttons.
'Social with moderation' will be a different medium.
It will change our relationship with the truth, hopefully for the better.

New media has always created post-truth realities and that's not always a bad thing.
Although the church was Gutenberg's best customer, printing also enabled the reformation.
(And one of Luther's complaints in 1517 was the sale of indulgences, which was also enabled by print tech).

Later Galileo would be arrested for publishing a book.
We say lots of bad things online, but we also say some good things that are controversial because they're true, and create change that is scary to the status quo but ultimately good.
It can be tempting to resist change and maintain the world in its current shape.
Eppur si muove.

---

## Notes
{:.no_toc}

{% include flatpixel.html %}
