---
layout: post
title: Four platforms compared as trust networks
author: Abe Winter
date: 2019-06-19
description: The internet is chaotic good monetized by lawful evil
---

Western governments are trying to recast online speech as a security issue but are having trouble doing it without sounding like China.

I'm not accusing anyone of wanting to censor (though I'm also not *not* accusing anyone). I think even conscientious neutrals are worried about how 'bad information' spreads online. The biggest stakeholders here are companies who make money from communication platforms -- 5 years ago they were into privacy and free speech, now they're willing to censor and tattle to be viewed as good citizens.

I don't think this is a speech *or* a security issue. The problem is trust and trust is its own thing. Media has always had to invent rules for itself to become trustworthy, and has always operated on a frontier of trust. Yes the internet is different in the 'how' but this isn't a new problem.

1. toc
{:toc}

## Trust networks

Internet platforms are different from what came before because trust is way more decentralized. Before ~2005, the major trust question for media consumers was 'trust in institutions'. In 2019 information still arrives **via** institutions like search engines, [apps bundled with your OS](https://www.apple.com/apple-news/), or your cable modem. But it's written by third parties and promoted by microinfluencers, people you follow, and behavior-based secret formulas.

Because of 'follow' and personalization features, every serious content distribution platform is a trust network now. But products in this space can be differentiated by:

* Do they clearly separate material that's from your trust network from material that's globally popular, or personalized via secret formula
* How do they handle second-degree trust or 'friends of friends'. For most products, the second-degree trust model is a recommendation model designed to increase engagement / sales / clicks
* Is trust binary? Or more complicated:
    - Negative trust, i.e. you can see information from people you disagree with marked as such
    - Is trusting someone 'global' or is it by category / topic -- i.e. can you trust someone positively on 'politics & war' and negatively on 'mathematics & philosophy'.

## Twitter

Twitter is the most successful example of single-hop trust. The app uses a secret formula to show you a feed of content from people you follow (albeit mixed with promoted and personalized content).

Because the format is so constrained, there's a good chance that someone is posting a link to something external to comment on it. Threaded conversations are often substantive and you can use global follower counts to decide how much to trust someone who's weighing in.

There's apparently a ['dislike' button](https://mashable.com/2017/04/13/twitter-has-a-dislike-tweet-option/) but it seems like the results aren't made public, so it's impossible to tell at a glance whether 'vote counts' are balanced. There's a big difference between '90% of people hate this' and '100 people like this'.

Second-degree trust only exists in the form of retweets.

## Reddit

Reddit has global trust scores for users (points and badges) and has two-way voting on posts and comments. Most posts are links to external content, i.e. people are posting for sharing and debate.

Subreddits are topic-based and I think they use rules and moderation to keep the conversation on topic. The smaller ones are small communities so there's an implicit trust network here but this isn't the same as marking how often you agree with 

Second-degree trust is nonexistent.

[Reddit has clear clear settings for disabling personalization](https://www.reddit.com/personalization) which is cool, though the involvement of 3rd parties is creepy.

## Youtube & FB

Even though I'm apparently [not moral enough to have a facebook account](https://youtu.be/m57gzA2JCcM?t=841), I know that engagement is the ride or die metric for both of these sites.

That means that both of them are using the behavior of people similar to you to predict what you'll like. There's also some sense of subscription if you're logged in but (1) what has to go wrong in your life to make you log into these products and (2) the content mix is always going to be out of your control.

FB repurposed last year to show you ['more content from your friends'](https://www.vox.com/2018/1/11/16881160/facebook-mark-zuckerberg-news-feed-algorithm-content-video-friends-family-media-publishers) which in theory is good from a trust perspective. I think it's actually *bad* because I don't want to see original content posted on facebook, I'd want to see external content in the context of what my network thinks about it.

## What am I doing about this 

I've written [trustgator, a link aggregator with a proof-of-concept trust network](https://github.com/abe-winter/trustgator). There's a screenshot on the readme of how this works. Check it out and run one yourself if you like the idea.
